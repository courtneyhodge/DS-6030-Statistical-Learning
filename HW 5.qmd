---
title: "Homework #5: Probability and Classification" 
author: "Courtney Hodge"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```


# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
dir_data= 'https://mdporter.github.io/teaching/data/' # data directory
library(glmnet)
library(tidyverse) # functions for data manipulation  
```


# Crime Linkage

Crime linkage attempts to determine if a set of unsolved crimes share a common offender. *Pairwise* crime linkage is the more simple task of deciding if two crimes share a common offender; it can be considered a binary classification problem. The linkage training data has 8 evidence variables that measure the similarity between a pair of crimes:

- `spatial` is the spatial distance between the crimes
- `temporal` is the fractional time (in days) between the crimes
- `tod` and `dow` are the differences in time of day and day of week between the crimes
- `LOC`, `POA,` and `MOA` are binary with a 1 corresponding to a match (type of property, point of entry, method of entry)
- `TIMERANGE` is the time between the earliest and latest possible times the crime could have occurred (because the victim was away from the house during the crime).
- The response variable indicates if the crimes are linked ($y=1$) or unlinked ($y=0$).


These problems use the [linkage-train](`r file.path(dir_data, "linkage_train.csv") `) and [linkage-test](`r file.path(dir_data, "linkage_test.csv") `) datasets (click on links for data). 


## Load Crime Linkage Data

::: {.callout-note title="Solution"}
```{r}
train_data <- read.csv("C:\\Users\\hodge\\Downloads\\linkage_train.csv")

test_data <- read.csv("C:\\Users\\hodge\\Downloads\\linkage_test.csv")
```

:::

# Problem 1: Penalized Regression for Crime Linkage

## a. Fit a penalized *linear regression* model to predict linkage. 

Use an elastic net penalty (including lasso and ridge) (your choice). 

- Report the value of $\alpha \in [0, 1]$ used. 
- Report the value of $\lambda$ used.
- Report the estimated coefficients.

::: {.callout-note title="Solution"}

```{r}
# separate features from response
X_train <- as.matrix(train_data[, c('spatial', 'temporal', 'tod', 'dow', 'LOC', 'POA', 'MOA', 'TIMERANGE')])
y_train <- train_data$y

#standardize the features (mean = 0, std = 1)
X_train_scaled <- scale(X_train)

#set a sequence of alpha values to try from Ridge regression (alpha = 0) to Lasso (alpha = 1)
alpha_values <- seq(0,1, by=0.1)

#initialize variables to store the best alpha and corresponding cv.glmnet model

best_alpha <- NULL
best_model <- NULL
best_lambda <- NULL
min_cv_error <- Inf  # Initialize with a large number to store the minimum error

for (alpha in alpha_values){
  #fit elastic net model w/ corss-val for the current alpha
  cv_fit <- cv.glmnet(X_train_scaled, y_train, alpha = alpha, nfolds = 5)
  
  #check if the current model has a lower cv error than the previous best model
  if(min(cv_fit$cvm) < min_cv_error){
    min_cv_error <- min(cv_fit$cvm)
    best_alpha <- alpha
    best_model <- cv_fit
    best_lambda <- cv_fit$lambda.min #lambda that minimizes cross-validation err
  }
}
```


```{r}
# Print the best alpha and lambda values
cat("Best alpha (mixing between Ridge and Lasso):", best_alpha, "\n")
cat("Best lambda (regularization strength):", best_lambda, "\n")

# Extract the estimated coefficients from the best model
coefficients <- coef(best_model, s = best_lambda)

# Print estimated coefficients for each feature
print(coefficients)
```

:::


## b. Fit a penalized *logistic regression* model to predict linkage. 

Use an elastic net penalty (including lasso and ridge) (your choice). 

- Report the value of $\alpha \in [0, 1]$ used. 
- Report the value of $\lambda$ used.
- Report the estimated coefficients.

::: {.callout-note title="Solution"}
```{r}
# separate features from response
X_train <- as.matrix(train_data[, c('spatial', 'temporal', 'tod', 'dow', 'LOC', 'POA', 'MOA', 'TIMERANGE')])
y_train <- train_data$y

#standardize the features (mean = 0, std = 1)
X_train_scaled <- scale(X_train)

#set a sequence of alpha
alpha_values <- seq(0, 1, by = 0.1)

#initialize variables to store the best alpha & corresponding cv.glmnet model
best_alpha <- NULL
best_model <- NULL
best_lambda <- NULL
min_cv_error <- Inf #initialize w/ large # to store the minimum error

#loop over each alpha value
for (alpha in alpha_values){
  # fit elastic net model w/ CV to find the best alph and lambda
  cv_fit <- cv.glmnet(X_train_scaled, y_train, alpha = alpha, family = "binomial", nfolds = 5)
  
  #check if the current model has a lower cross-validation error than the previous best
  if(min(cv_fit$cvm) < min_cv_error){
    min_cv_error <- min(cv_fit$cvm)
    best_alpha <- alpha
    best_model <- cv_fit
    best_lambda <- cv_fit$lambda.min #lamda that minimizes cross-validation error
  }
}

```

```{r}
# Print the best alpha and lambda values
cat("Best alpha (mixing between Ridge and Lasso):", best_alpha, "\n")
cat("Best lambda (regularization strength):", best_lambda, "\n")

#extract coefficients of the best model
coefficients <- coef(best_model, s = best_lambda)

cat("Estimated Coefficients for each feature:")
#print estimated coefficients for each feature
print(coefficients)
```

:::

# Problem 2: Random Forest for Crime Linkage

Fit a random forest model to predict crime linkage. 

- Report the loss function (or splitting rule) used. 
- Report any non-default tuning parameters.
- Report the variable importance (indicate which importance method was used). 

::: {.callout-note title="Solution"}
```{r}
library(randomForest)

#fit a RF model w/ default parameters
rf_model <- randomForest(X_train, y_train, ntree = 500, mtry = sqrt(ncol(X_train)), importance = TRUE)

```


```{r}

#report the loss function (splitting rule) - classification, it's Gini
cat("Splitting rule (loss function) used: Gini impurity\n")

# Non-default tuning parameters (if any)
cat("Number of trees (ntree):", rf_model$ntree, "\n")
cat("Number of variables tried at each split (mtry):", rf_model$mtry, "\n")

#variable importance
importance_values <- importance(rf_model)
cat("Variable Importance (Mean Decrease in Gini):\n")
print(importance_values)

#plot the variable importance
varImpPlot(rf_model)
```

> It looks like the spatial distance between the crimes is the most important variable for predicting crime linkage, followed by the fractional time (in days) between the crimes.

:::

# Problem 3: ROC Curves

## a. ROC curve: training data

Produce one plot that has the ROC curves, using the *training data*, for all three models (linear, logistic, and random forest). Use color and/or linetype to distinguish between models and include a legend.    
Also report the AUC (area under the ROC curve) for each model. Again, use the *training data*. 

- Note: you should be weary of being asked to evaluation predictive performance from the same data used to estimate the tuning and model parameters. The next problem will walk you through a more proper way of evaluating predictive performance with resampling. 

::: {.callout-note title="Solution"}
```{r}
library(pROC) #for ROC and AUC

#Separate features and response
X_train <- as.matrix(train_data[, c('spatial', 'temporal', 'tod', 'dow', 'LOC', 'POA', 'MOA', 'TIMERANGE')])

y_train <- train_data$y #response is a binary variable

#fit linear regression (Elastic Net)
alpha_linear <- 0.5 #choose a 50/50 mix of ridge and lasso for elastic net
cv_fit_linear <- cv.glmnet(X_train_scaled, y_train, alpha = alpha_linear, family = "gaussian")
linear_preds <- predict(cv_fit_linear, newx = X_train_scaled, s = "lambda.min")

#convert linear reg output to probabilities (since it's not a binary classifier)
linear_probs <-1/(1+exp(-linear_preds)) #sigmoid function to convert to probabilities

#fit Logistic Regression (elastic net)
cv_fit_logistic <- cv.glmnet(X_train_scaled, y_train, alpha = alpha_linear, family = "binomial")
logistic_probs <- predict(cv_fit_logistic, newx = X_train_scaled, s = "lambda.min", type = "response")

#fit random forest
rf_model <- randomForest(X_train, as.factor(y_train), ntree = 500, mtry = sqrt(ncol(X_train)))
rf_probs <- predict(rf_model, newdata = X_train, type = "prob")[,2] #get probabilities for class 1 (linked)

#compute the ROC and AUC for all models
roc_linear <- roc(y_train, linear_probs)
roc_logistic <- roc(y_train, logistic_probs)
roc_rf <- roc(y_train, rf_probs)

#AUC Vals
auc_linear <- auc(roc_linear)
auc_logistic <- auc(roc_logistic)
auc_rf <- auc(roc_rf)

cat("AUC for Linear Regression:", auc_linear, "\n")
cat("AUC for Logistic Regression:", auc_logistic, "\n")
cat("AUC for Random Forest:", auc_rf, "\n")

```

```{r}
ggplot() +
  geom_line(aes(x = roc_linear$specificities, y = roc_linear$sensitivities, color = "Linear Regression"), size = 1) +
  geom_line(aes(x = roc_logistic$specificities, y = roc_logistic$sensitivities, color = "Logistic Regression"), size = 1) +
  geom_line(aes(x = roc_rf$specificities, y = roc_rf$sensitivities, color = "Random Forest"), size = 1) +
  labs(x = "1 - Specificity", y = "Sensitivity (True Positive Rate)", title = "ROC Curves for Linear, Logistic, and Random Forest Models") +
  scale_color_manual(values = c("blue", "green", "red")) + 
  theme_minimal() +
  theme(legend.title = element_blank())

```


:::


## b. ROC curve: resampling estimate

Recreate the ROC curve from the penalized logistic regression (logreg) and random forest (rf) models using repeated hold-out data. The following steps will guide you:

- For logreg, use $\alpha=.75$. For rf use *mtry = 2*,  *num.trees = 1000*, and fix any other tuning parameters at your choice. 
- Run the following steps 25 times:
    i. Hold out 500 observations.
    ii. Use the remaining observations to estimate $\lambda$ using 10-fold CV for the logreg model. Don't tune any rf parameters.
    iii. Predict the probability of linkage for the 500 hold-out observations.
    iv. Store the predictions and hold-out labels.
    v. Calculate the AUC. 
- Report the mean AUC and standard error for both models. Compare to the results from part a. 
- Produce two plots showing the 25 ROC curves for each model. 
- Note: by estimating $\lambda$ each iteration, we are incorporating the uncertainty present in estimating that tuning parameter. 
    
::: {.callout-note title="Solution"} 
```{r}
# Separate features and response
X <- as.matrix(train_data[, c('spatial', 'temporal', 'tod', 'dow', 'LOC', 'POA', 'MOA', 'TIMERANGE')])
y <- train_data$y  # Assuming response is binary (0 = unlinked, 1 = linked)

#parameters
alpha_logreg <- 0.75 #logistic regression alpha
num_resamples <- 25 #number of resampling iterations
holdout_size <- 500 #size of hold-out set

#store results
auc_logreg <- numeric(num_resamples)
auc_rf <- numeric(num_resamples)
roc_logreg_list <- list() # to store ROC curves for logref
roc_rf_list <- list() #to store ROC curves for random forest

set.seed(123) #for reproducibility

#resampling loop
for(i in 1:num_resamples){
  # Step i: Randomly split the data into hold-out and training sets
  holdout_indices <- sample(1:nrow(X), holdout_size)
  X_holdout <- X[holdout_indices, ]
  y_holdout <- y[holdout_indices]
  X_train <- X[-holdout_indices, ]
  y_train <- y[-holdout_indices]
  
  # Step ii: Logistic Regression (with 10-fold CV to estimate lambda)
  X_train_scaled <- scale(X_train)  # Standardize the training data
  cv_fit_logreg <- cv.glmnet(X_train_scaled, y_train, alpha = alpha_logreg, family = "binomial", nfolds = 10)
  lambda_min <- cv_fit_logreg$lambda.min  # Get the best lambda
  
  #predict probabilities for the hold-out set
  X_holdout_scaled <- scale(X_holdout) #standardize the hold-out data
  logreg_probs <- predict(cv_fit_logreg, newx = X_holdout_scaled, s = lambda_min, type = "response")
  
  #step iii: Random Forest
   rf_model <- randomForest(X_train, as.factor(y_train), ntree = 1000, mtry = 2)
  rf_probs <- predict(rf_model, newdata = X_holdout, type = "prob")[,2]  # Get probabilities for class 1
  
  # Step iv: Calculate AUC for each model
  roc_logreg <- roc(y_holdout, logreg_probs)
  roc_rf <- roc(y_holdout, rf_probs)
  
  auc_logreg[i] <- auc(roc_logreg)
  auc_rf[i] <- auc(roc_rf)
  
  # Store the ROC curves
  roc_logreg_list[[i]] <- roc_logreg
  roc_rf_list[[i]] <- roc_rf
}

#step v: calculate mean AUC and standard error
mean_auc_logreg <- mean(auc_logreg)
se_auc_logreg <- sd(auc_logreg) / sqrt(num_resamples)
mean_auc_rf <- mean(auc_rf)
se_auc_rf <- sd(auc_rf) / sqrt(num_resamples)

```


```{r}
#report results
cat("Logistic Regression (alpha = 0.75):\n")
cat("Mean AUC:", mean_auc_logreg, "\n")
cat("Standard Error:", se_auc_logreg, "\n\n")

cat("Random Forest (mtry = 2, ntree = 1000):\n")
cat("Mean AUC:", mean_auc_rf, "\n")
cat("Standard Error:", se_auc_rf, "\n")

#step vi: Plot ROC curves for both models
#logistic regression ROC Plot
ggplot() +
  lapply(roc_logreg_list, function(roc) {
    geom_line(aes(x = 1 - roc$specificities, y = roc$sensitivities), color = "blue", alpha = 0.3)
  }) +
  labs(x = "1 - Specificity", y = "Sensitivity", title = "ROC Curves for Penalized Logistic Regression") +
  theme_minimal()

# Random Forest ROC Plot
ggplot() +
  lapply(roc_rf_list, function(roc) {
    geom_line(aes(x = 1 - roc$specificities, y = roc$sensitivities), color = "red", alpha = 0.3)
  }) +
  labs(x = "1 - Specificity", y = "Sensitivity", title = "ROC Curves for Random Forest") +
  theme_minimal()
```

:::

# Problem 4: Contest

## a. Contest Part 1: Predict the estimated *probability* of linkage. 

Predict the estimated *probability* of linkage for the test data (using any model). 

- Submit a .csv file (ensure comma separated format) named `lastname_firstname_1.csv` that includes the column named **p** that is your estimated posterior probability. We will use automated evaluation, so the format must be exact. 
- You are free to any model (even ones we haven't yet covered in the course).
- You are free to use any data transformation or feature engineering.
- You will receive credit for a proper submission; the top five scores will receive 2 bonus points.     
- Your probabilities will be evaluated with respect to the mean negative Bernoulli log-likelihood (known as the average *log-loss* metric):
$$ 
L = - \frac{1}{M} \sum_{i=1}^m [y_i \log \, \hat{p}_i + (1 - y_i) \log \, (1 - \hat{p}_i)]
$$
where $M$ is the number of test observations, $\hat{p}_i$ is the prediction for the $i$th test observation, and $y_i \in \{0,1\}$ are the true test set labels. 

::: {.callout-note title="Solution"}
```{r}
# Fit Elastic Net Logistic Regression (alpha = 0.75)
alpha_logreg <- 0.75
cv_fit_logreg <- cv.glmnet(X_train_scaled, y_train, alpha = alpha_logreg, family = "binomial", nfolds = 10)
best_lambda <- cv_fit_logreg$lambda.min  # Get the best lambda from cross-validation

# Preprocess the test data similarly (ensure the same scaling)
X_test <- as.matrix(test_data[, c('spatial', 'temporal', 'tod', 'dow', 'LOC', 'POA', 'MOA', 'TIMERANGE')])
X_test_scaled <- scale(X_test, center = attr(X_train_scaled, "scaled:center"), scale = attr(X_train_scaled, "scaled:scale"))

# Load randomForest library
library(randomForest)

# Train Random Forest model
rf_model <- randomForest(X_train, as.factor(y_train), ntree = 1000, mtry = 2)

# Predict probabilities on test set
rf_probs <- predict(rf_model, newdata = X_test, type = "prob")[,2]  # Probability for class 1

# Save the predictions to a CSV file
output_rf_df <- data.frame(p = rf_probs)

colnames(output_rf_df)[1] <- "p"

write.csv(output_rf_df, file = "hodge_courtney_1.csv", row.names = FALSE)
```
:::


## b. Contest Part 2: Predict the *linkage label*. 

Predict the linkages for the test data (using any model). 

- Submit a .csv file (ensure comma separated format) named `lastname_firstname_2.csv` that includes the column named **linkage** that takes the value of 1 for linked pairs and 0 for unlinked pairs. We will use automated evaluation, so the format must be exact. 
- You are free to any model (even ones we haven't yet covered in the course).
- You are free to use any data transformation or feature engineering.
- Your labels will be evaluated based on total cost, where cost is equal to `1*FP + 8*FN`. This implies that False Negatives (FN) are 8 times as costly as False Positives (FP).    
- You will receive credit for a proper submission; the top five scores will receive 2 bonus points. Note: you only will get bonus credit for one of the two contests. 

::: {.callout-note title="Solution"}
```{r}
#Fit elastic net logistic regression 
alpha_logreg <- 0.75
cv_fit_logreg <- cv.glmnet(X_train_scaled, y_train, alpha = alpha_logreg, family = "binomial", nfolds = 10)
best_lambda <- cv_fit_logreg$lambda.min #get the best lambda from Cross Val

#predict probabilities for the test set
test_probs <- predict(cv_fit_logreg, newx = X_test_scaled, s= best_lambda, type = "response")

#set a custom threshold to minimize False Negatives
threshold <- 0.3 

#predict linkage labels based on the adjusted threshold
test_labels <- ifelse(test_probs > threshold, 1, 0)

#create the output dataframe with a single column 'linkage' containing the labels
output_df <- data.frame(linkage = test_labels)

colnames(output_df)[1] <- "linkage"

#save the predictions to a CSV file
write.csv(output_df, file = "hodge_courtney_2.csv", row.names = FALSE)
```

:::
