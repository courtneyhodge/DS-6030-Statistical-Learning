---
title: "Homework #1: Supervised Learning"
author: "Courtney Hodge"
format: ds6030hw-html
---

```{r config}
#| include: false
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

# Required R packages and Directories {.unnumbered .unlisted}

```{r packages}
#| message: false
#| warning: false
library(tidyverse) # functions for data manipulation
library(tibble)
```

# Problem 1: Evaluating a Regression Model

## a. Data generating functions

Create a set of functions to generate data from the following distributions:

```{=tex}
\begin{align*}
X &\sim \mathcal{N}(0, 1) \\
Y &= -1 + .5X + .2X^2 + \epsilon \\
\epsilon &\sim \mathcal{N}(0,\, \sigma)
\end{align*}
```
::: {.callout-note title="Solution"}
```{r}
sim_x <- function(n) rnorm(n)         #U[0,1]
f <-function(x) -1 + 0.5*x + 0.2*(x^2)  #true mean function
sim_y <- function(x, sd) {            #generate Y|X from N{f(x), sd}
  n = length(x)
  f(x) + rnorm(n, mean = 0, sd = sd)
}
```
:::

## b. Generate training data

Simulate $n=100$ realizations from these distributions using $\sigma=3$. Produce a scatterplot and draw the true regression line $f(x) = E[Y \mid X=x]$.

-   Use `set.seed(611)` prior to generating the data.

::: {.callout-note title="Solution"}
```{r}
#-- Settings
n = 100       #number of observations
sd = 3        #stdev for error

set.seed(611) # set seed for reproducibility
x = sim_x(n)  # get x values
y = sim_y(x, sd = sd) # get y values
data_train = tibble(x, y) # training data tibble

#----------DELETE ME-------------#
#-- Generate Test Data
ntest = 50000 # Number of test samples
set.seed(611) # set *different* seed
xtest = sim_x(ntest) # generate test X's
ytest = sim_y(xtest, sd=sd) # generate test Y's
data_test = tibble(x=xtest, y=ytest) # test data

```

```{r}
#-- plot points and true regression function
scatter_plot <- ggplot(data_train, aes(x,y)) +
geom_point() +
geom_function(fun = f)
scatter_plot
```
:::

## c. Fit three models

Fit three polynomial regression models using least squares: linear, quadratic, and cubic. Produce another scatterplot, add the fitted lines and true population line $f(x)$ using different colors, and add a legend that maps the line color to a model.

-   Note: The true model is quadratic, but we are also fitting linear (less complex) and cubic (more complex) models.

::: {.callout-note title="Solution"}
```{r}
linear_model <- lm(y~poly(x,1), data = data_train)
quadratic_model <-lm(y~poly(x,2), data = data_train)
cubic_model <-lm(y~poly(x,3), data=data_train)

x_seq <- seq(min(data_train$x), max(data_train$x), length.out = 100)


# Assuming sim_y without error (sd=0) represents the true quadratic model
y_true <- sim_y(x_seq, sd = 0)  # True quadratic model without noise

y_linear_pred <- predict(linear_model, newdata = data.frame(x = x_seq))
y_quadratic_pred <- predict(quadratic_model, newdata = data.frame(x = x_seq))
y_cubic_pred <- predict(cubic_model, newdata = data.frame(x = x_seq))
```

```{r}
# Create a sequence of x values for plotting the fitted lines
x_seq <- seq(min(data_train$x), max(data_train$x), length.out = 100)

# Combine the predicted values into one data frame for plotting
fit_data <- tibble(
  x = rep(x_seq, 4),
  y = c(y_linear_pred, y_quadratic_pred, y_cubic_pred, y_true),
  model = factor(rep(c("Linear Model", "Quadratic Model", "Cubic Model", "True Model"), each = 100))
)

# Create the ggplot
ggplot(data_train, aes(x = x, y = y)) +
  geom_point() +  # Scatter plot of the data
  geom_line(data = fit_data, aes(x = x, y = y, color = model)) +  # Fitted lines
  labs(title = "Polynomial Regression Models", x = "x", y = "y") +
  scale_color_manual(values = c("red", "blue", "purple", "green"))
```
:::

## d. Predictive performance

Generate a *test data* set of 10,000 observations from the same distributions. Use `set.seed(612)` prior to generating the test data.

-   Calculate the estimated mean squared error (MSE) for each model.
-   Are the results as expected?

::: {.callout-note title="Solution"}
```{r}
sim_x <- function(n) runif(n)         #U[0,1]
f <-function(x) -1 + 0.5*x + 0.2*x^2  #true mean function
sim_y <- function(x, sd) {            #generate Y|X from N{f(x), sd}
  n = length(x)
  f(x) + rnorm(n, mean = 0, sd = sd)
}

#-- Generate Test Data
ntest = 10000 # Number of test samples
set.seed(612) # set *different* seed
xtest = sim_x(ntest) # generate test X's
ytest = sim_y(xtest, sd=sd) # generate test Y's
data_test = tibble(x=xtest, y=ytest) # test data


linear_yhat = predict(linear_model, data_test)
quadratic_yhat = predict(quadratic_model, data_test)
cubic_yhat = predict(cubic_model, data_test)

#: calculate test MSE
linear_mse_test = mean( (data_test$y - linear_yhat)^2 )
quadratic_mse_test = mean((data_test$y - quadratic_yhat) ^2 )
cubic_mse_test = mean((data_test$y - cubic_yhat) ^ 2)

linear_mse_test
quadratic_mse_test
cubic_mse_test

```

> The results are as expected. The linear MSE is the lowest but is very close to the quadratic mse. The difference is very small though. The Cubic MSE is larger than linear and quadratic's showing that the cubic model might be overfitting b/c of unnecessary complexity. The noise is captured rather than the true underlying relationship.
:::

## e. Optimal performance

What is the best achievable MSE? That is, what is the MSE if the true $f(x)$ was used to evaluate the test set? How close does the best method come to achieving the optimum?

::: {.callout-note title="Solution"}

```{r}
true_yhat <- sapply(data_test$x, function(x) -1 + 0.5*x + 0.2*(x^2))

true_mse_test = mean((data_test$y - true_yhat) ^2)

true_mse_test
```

> The best achievable MSE are supposed to be quadratic and linear because they should come closer to the best achievable MSE of 8.874721. Cubic is in this case should be higher than all three mse values, because of its excessive complexity. Since the linear model performs with an MSE of 9.288 and the quadratic model performs with an MSE of 9.447, they are therefore very close at achieving the optimum.
:::

## f. Replication

The MSE scores obtained in part *d* came from one realization of training data. Here will we explore how much variation there is in the MSE scores by replicating the simulation many times.

-   Re-run parts b. and c. (i.e., generate training data and fit models) 100 times.
    -   Do not generate new testing data
    -   Use `set.seed(613)` prior to running the simulation and do not set the seed in any other places.
-   Calculate the test MSE for all simulations.
    -   Use the same test data from part d. (This question is only about the variability that comes from the *training data*).
-   Create kernel density or histogram plots of the resulting MSE values for each model.

::: {.callout-note title="Solution"}
```{r}
#--function to calculate MSE
calc_mse <- function(model, data_test){
  y_pred <- predict(model, newdata = data_test)
  mean((data_test$y - y_pred)^2)
}

#--set up num of simulations
n_simulations = 100

#--set the simulation seed
set.seed(613)

#--Create vector to store MSE values for each model
mse_linear <- numeric(n_simulations)
mse_quadratic <- numeric(n_simulations)
mse_cubic <- numeric(n_simulations)

#-- start the simulation
for (i in 1:n_simulations){
  #generate new training data each time
  x_train = sim_x(100)
  y_train = sim_y(x_train, sd = 3)
  data_train = tibble(x = x_train, y = y_train)
  
  #fit the models
  linear_model <- lm(y~ poly(x, 1), data = data_train)
  quadratic_model <- lm(y~poly(x,2), data = data_train)
  cubic_model <- lm(y~poly(x,3), data = data_train)
  
  #calculate the MSE for each model using the test data
  mse_linear[i] <-calc_mse(linear_model, data_test)
  mse_quadratic[i] <-calc_mse(quadratic_model, data_test)
  mse_cubic[i] <- calc_mse(cubic_model, data_test)
}
```

```{r}
#--plot the MSE distributions
mse_data <- tibble(
  MSE = c(mse_linear, mse_quadratic, mse_cubic),
  Model = rep(c("Linear", "Quadratic", "Cubic"), each = n_simulations)
)

# Kernel Density Plot
ggplot(mse_data, aes(x = MSE, fill = Model)) +
  geom_density(alpha = 0.5) +
  labs(title = "Kernel Density of MSE for Linear, Quadratic, and Cubic Models",
       x = "MSE", y = "Density") +
  scale_fill_manual(values = c("red", "blue", "green"))+
  xlim(8.5, 11)
```

```{r}
#--histogram plot
ggplot(mse_data, aes(x = MSE, fill = Model)) +
  geom_histogram(bins = 30, position = "identity", alpha = 0.5) +
  labs(title = "Histogram of MSE for Linear, Quadratic, and Cubic Models",
       x = "MSE", y = "Frequency") +
  scale_fill_manual(values = c("red", "blue", "green"))+
  xlim(8.5, 11)

```
:::

## g. Best model

Show a count of how many times each model was the best. That is, out of the 100 simulations, count how many times each model had the lowest MSE.

::: {.callout-note title="Solution"}
```{r}
# Combine the MSEs for all simulations into a data frame
mse_results <- tibble(
  Simulation = 1:n_simulations,
  Linear = mse_linear,
  Quadratic = mse_quadratic,
  Cubic = mse_cubic
)

# Find the model with the lowest MSE for each simulation
mse_results <- mse_results %>%
  rowwise() %>%
  mutate(Best_Model = case_when(
    Linear == min(c(Linear, Quadratic, Cubic)) ~ "Linear",
    Quadratic == min(c(Linear, Quadratic, Cubic)) ~ "Quadratic",
    Cubic == min(c(Linear, Quadratic, Cubic)) ~ "Cubic"
  )) %>%
  ungroup()

# Count how many times each model was the best
best_model_count <- mse_results %>%
  count(Best_Model)

# Print the count of the best models
print(best_model_count)
```
:::

## h. Function to implement simulation

Write a function that implements the simulation in part *f*. The function should have arguments for i) the size of the training data $n$, ii) the standard deviation of the random error $\sigma$, and iii) the test data. Use the same `set.seed(613)`.

::: {.callout-note title="Solution"}
```{r}
simulation_f <- function(n, sigma, data_test){
  set.seed(613)
  
  # Function to calculate MSE
  calc_mse <- function(model, data_test) {
    y_pred <- predict(model, newdata = data_test)
    mean((data_test$y - y_pred)^2)
  }
  
  # Initialize vectors to store MSE results for each simulation
  #n_simulations <- 100
  mse_linear <- numeric(n)
  mse_quadratic <- numeric(n)
  mse_cubic <- numeric(n)
  
  
  # Run the simulation for each iteration
  for (i in 1:100) {
    # Generate new training data
    x_train <- sim_x(n)
    y_train <- sim_y(x_train, sd = sigma)
    data_train <- tibble(x = x_train, y = y_train)
    
    # Fit models: linear, quadratic, cubic
    linear_model <- lm(y ~ poly(x, 1), data = data_train)
    quadratic_model <- lm(y ~ poly(x, 2), data = data_train)
    cubic_model <- lm(y ~ poly(x, 3), data = data_train)
    
    # Calculate MSE for each model
    mse_linear[i] <- calc_mse(linear_model, data_test)
    mse_quadratic[i] <- calc_mse(quadratic_model, data_test)
    mse_cubic[i] <- calc_mse(cubic_model, data_test)
  }
  
  #--part g, but I'm adding it here anyways
  # Combine results into a data frame
  mse_results <- tibble(
    Simulation = 1:n,
    Linear = mse_linear,
    Quadratic = mse_quadratic,
    Cubic = mse_cubic
  )
  
   # Determine the best model for each simulation and store the best MSE
  mse_results <- mse_results %>%
    rowwise() %>%
    mutate(
      Best_Model = case_when(
        Linear == min(c(Linear, Quadratic, Cubic)) ~ "Linear",
        Quadratic == min(c(Linear, Quadratic, Cubic)) ~ "Quadratic",
        Cubic == min(c(Linear, Quadratic, Cubic)) ~ "Cubic"
      ),
      Best_MSE = min(c(Linear, Quadratic, Cubic))
    ) %>%
    ungroup()
  
   # Count how many times each model was the best
  best_model_count <- mse_results %>%
    count(Best_Model)
  
  # Return a list with detailed results
  return(list(
    mse_results = mse_results,
    best_model_count = best_model_count
  ))
}
```
:::

## i. Performance when $\sigma=2$

Use your function to repeat the simulation in part *f*, but use $\sigma=2$. Report the number of times each model was best (you do not need to produce any plots).

-   Be sure to generate new test data with ($n = 10000$, $\sigma = 2$, using `seed = 612`).

::: {.callout-note title="Solution"}
```{r}
#-- Generate Test Data
ntest = 10000 # Number of test samples
sigma = 100
set.seed(612) # set *different* seed
xtest = sim_x(ntest) # generate test X's
ytest = sim_y(xtest, sd=sigma) # generate test Y's
data_test = tibble(x=xtest, y=ytest) # test data

simulation_result <- simulation_f(n = 100, sigma, data_test)

simulation_result$best_model_count
```

:::

## j. Performance when $\sigma=4$ and $n=300$

Repeat *i*, but now use $\sigma=4$ and $n=300$.

-   Be sure to generate new test data with ($n = 10000$, $\sigma = 4$, using `seed = 612`).

::: {.callout-note title="Solution"}
```{r}
#-- Generate Test Data
ntest = 10000 # Number of test samples
sigma = 4
set.seed(612) # set *different* seed
xtest = sim_x(ntest) # generate test X's
ytest = sim_y(xtest, sd=sigma) # generate test Y's
data_test = tibble(x=xtest, y=ytest) # test data

simulation_result <- simulation_f(n= 300 , sigma, data_test)

simulation_result$best_model_count
#c12,l8,q80
```


:::

## k. Understanding

Describe the effects $\sigma$ and $n$ has on selection of the best model? Why is the *true* model form (i.e., quadratic) not always the *best* model to use when prediction is the goal?

::: {.callout-note title="Solution"}
> depending on n, which is the size of the training data, and sigma, which is the standard deviation of the random error, the higher the n value, the more simulations we'll have, resulting in a higher count of how many times each model performed the best in a specific simulation. This means that distribution of this "best model" count will grow in respect to n. In regard to sigma, with a higher sigma, we see the slightest shift in the "best model" count go from linear to quadratic, but the best model over n simulations is almost always Linear. Why the true model is not always the best model to use when prediction is the goal is likely due to the Bias-Variance Tradeoff. The Linear Model has lower variance and high bias, while the quadratic model has more of a balanced bias-variance.
:::
